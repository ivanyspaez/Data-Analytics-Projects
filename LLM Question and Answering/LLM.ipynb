{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!python3 --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cc07v5thlHjs","executionInfo":{"status":"ok","timestamp":1686592885869,"user_tz":300,"elapsed":16,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"40f0d934-ec81-49c7-9463-8932d36d36ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}]},{"cell_type":"code","source":["!pip install fire\n","!pip install gradio\n","!pip install transformers\n","!pip install git+https://github.com/huggingface/peft.git\n","!pip install sentencepiece\n","!pip install accelerate\n","!pip install bitsandbytes\n","!pip install langchain\n","!pip install sentence_transformers\n","!pip install chromadb\n","!pip install xformers"],"metadata":{"id":"8IJEOS5Sd3Os"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## **Carga de los datos:**\n","\n","Despues de procesados los datos con los primeros script, se obtiene un archivo con los parrafos correspondientes a las resoluciones en bruto. Estos deben someterse a un pequeño reporceso en el cual se realiza:\n","\n","*   Eliminación de caracteres no necesarios (**Tales como:** Tildes, dieresis, virgulillas entre otros)\n","*   Separación en diccionario, donde se colocan los contenidos de cada resolución en una llave del diccionario.\n","\n","---"],"metadata":{"id":"2RYIP413ZPSD"}},{"cell_type":"code","source":["!apt-get install wget\n","!wget -O resultado.txt \"https://raw.githubusercontent.com/bjportelac/UP-0001-MainCodeAndData/main/resultado.txt\""],"metadata":{"id":"0ivFH3Y2TkSA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686593076894,"user_tz":300,"elapsed":5579,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"a4a0f056-e04c-448f-8c36-2bcbb29edfba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","wget is already the newest version (1.20.3-1ubuntu2).\n","0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n","--2023-06-12 18:04:35--  https://raw.githubusercontent.com/bjportelac/UP-0001-MainCodeAndData/main/resultado.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 55811 (55K) [text/plain]\n","Saving to: ‘resultado.txt’\n","\n","resultado.txt       100%[===================>]  54.50K  --.-KB/s    in 0.002s  \n","\n","2023-06-12 18:04:35 (30.6 MB/s) - ‘resultado.txt’ saved [55811/55811]\n","\n"]}]},{"cell_type":"code","source":["import os\n","import unicodedata\n","\n","def processer(fileName:str, divider: str):\n","  \"\"\"\n","  Process a file and return a dictionary containing paragraphs for each key.\n","  Args:\n","      fileName (str): The name of the file to process.\n","      divider (str): The string used to divide paragraphs.\n","  Returns:\n","      dict: A dictionary containing keys and list of paragraphs for each key.\n","  \"\"\"\n","  dictionary = {}\n","  with open(fileName,'r',encoding='latin-1') as archive:\n","    lines = archive.readlines()\n","\n","  i = 0\n","  while i < len(lines):\n","    line = lines[i].strip()\n","    if line.startswith(\"Archivo:\"):\n","      key = line.split(\":\")[1].strip()\n","      dictionary[key] = []\n","\n","    elif key is not None and not lines[i].startswith(divider) and not lines[i].startswith('Contenido:'):\n","      parragraph = lines[i].strip()\n","      if(parragraph and parragraph != divider):\n","        dictionary[key].append(parragraph)\n","\n","    i+=1\n","\n","  return dictionary\n","\n","def stringRegularizer(wordList:list):\n","  \"\"\"\n","  Regularize a list of strings by normalizing, lowercasing, and capitalizing the first letter.\n","  Args:\n","      wordList (list): A list of strings to regularize.\n","  Returns:\n","      list: A list of regularized strings.\n","  \"\"\"\n","  regularized = []\n","  for string in wordList:\n","    string = unicodedata.normalize('NFKD',string).encode('ASCII','ignore').decode('utf-8')\n","    string = string.lower().strip()\n","    string = string.title()\n","    regularized.append(string)\n","\n","  return regularized\n","\n","def dictionaryCleaner(dictionary:dict):\n","  \"\"\"\n","  Clean a dictionary by regularizing its values.\n","  Args:\n","      dictionary (dict): A dictionary with keys and list of strings as values.\n","  Returns:\n","      dict: A cleaned dictionary with regularized values.\n","  \"\"\"\n","  for key in dictionary:\n","    value = dictionary[key]\n","    new_value = stringRegularizer(wordList=value)\n","    dictionary[key] = new_value\n","\n","  return dictionary\n","\n","\n","file_name =\"resultado.txt\"\n","divider = '-----------------------------'\n","\n","dictionary = processer(fileName=file_name,divider=divider)\n","dictionary = dictionaryCleaner(dictionary=dictionary)\n","\n","result = \"\"\n","\n","for key, values in dictionary.items():\n","    result += key + \": \"\n","    result += \", \".join(values)\n","    result += \"\\n\"\n","\n","# Write the result to a text file\n","with open(\"Parsed_regularized.txt\", \"w\") as file:\n","    file.write(result)\n","\n"],"metadata":{"id":"-L3TFw07Y9Q_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## **Instalación de requerimentos:**\n","---\n","\n","Ya que se debe trabajar con **LangChain** y **Chroma** se trae la importación e unstalacion de langChain para Python."],"metadata":{"id":"X4ksI0Vu4G-s"}},{"cell_type":"code","source":["#Activar CUDA\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQj5mSC_KZbZ","executionInfo":{"status":"ok","timestamp":1686593094649,"user_tz":300,"elapsed":398,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"68255441-4c13-435c-d29a-85e3f1d4ff35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Jun 12 18:04:53 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!nvcc --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IM2-adu2KvsU","executionInfo":{"status":"ok","timestamp":1686593097803,"user_tz":300,"elapsed":532,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"c74ce03b-a0db-4ce6-d36a-90675a57cf93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}]},{"cell_type":"markdown","source":["### 2. Carga del modelo:\n","\n","* Debido a que no existen muchos modelos que funcionen con el lenguaje español, se debe trabajar con uno que tenga cierta compatibilidad , en este caso se trabajara con el modelo **Alpaca LoRA 7B** el cual es de los que tienen mayor compatibilidad.\n","\n","* **1.1: Traer el repositorio de Alpaca LoRA** [https://github.com/tloen/alpaca-lora/]"],"metadata":{"id":"0oNeCpXmRQz8"}},{"cell_type":"code","source":["# Clonar el Repo\n","! git clone https://github.com/tloen/alpaca-lora.git"],"metadata":{"id":"6hFNChIzR5C3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686593112855,"user_tz":300,"elapsed":6318,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"18d8b043-00c4-4e3a-c7c4-ac003eea6aa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'alpaca-lora'...\n","remote: Enumerating objects: 607, done.\u001b[K\n","remote: Counting objects: 100% (51/51), done.\u001b[K\n","remote: Compressing objects: 100% (32/32), done.\u001b[K\n","remote: Total 607 (delta 28), reused 33 (delta 19), pack-reused 556\u001b[K\n","Receiving objects: 100% (607/607), 27.78 MiB | 6.57 MiB/s, done.\n","Resolving deltas: 100% (360/360), done.\n"]}]},{"cell_type":"code","source":["#Inspeccionar la carpeta del repositorio\n","%cd alpaca-lora"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NPjrfD0GBRE-","executionInfo":{"status":"ok","timestamp":1686593122346,"user_tz":300,"elapsed":411,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"655246cc-c39d-4145-a6e7-236d9c67fdf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/alpaca-lora\n"]}]},{"cell_type":"code","source":["#Listar los contenidos\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_UPS1lZJBcLd","executionInfo":{"status":"ok","timestamp":1686593124797,"user_tz":300,"elapsed":444,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"75c01391-f02a-458f-8cf2-f7db4ba64185"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["alpaca_data_cleaned_archive.json  generate.py\n","alpaca_data_gpt4.json             lengths.ipynb\n","alpaca_data.json                  LICENSE\n","DATA_LICENSE                      pyproject.toml\n","docker-compose.yml                README.md\n","Dockerfile                        requirements.txt\n","export_hf_checkpoint.py           \u001b[0m\u001b[01;34mtemplates\u001b[0m/\n","export_state_dict_checkpoint.py   \u001b[01;34mutils\u001b[0m/\n","finetune.py\n"]}]},{"cell_type":"code","source":["# Instalar los requerimientos del modelo\n","!pip install -r requirements.txt\n","%cd .."],"metadata":{"id":"OQoHRrdES0NM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import tensorflow as tf"],"metadata":{"id":"EW26OckRIhCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjrEn57uMK9g","executionInfo":{"status":"ok","timestamp":1686593201929,"user_tz":300,"elapsed":416,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"0af4df48-4162-47cf-fc48-dc3aa629e756"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["tf.test.gpu_device_name()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"X7EX5GBoMOC_","executionInfo":{"status":"ok","timestamp":1686593211902,"user_tz":300,"elapsed":7588,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"c8538557-d6e0-475f-f981-067fcfd2c89e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["%cd content\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cgW7U3eWPoV3","executionInfo":{"status":"ok","timestamp":1686593214243,"user_tz":300,"elapsed":13,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"a46b72d4-cbd6-44d4-95df-42a4315b055b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'content'\n","/content\n","\u001b[0m\u001b[01;34malpaca-lora\u001b[0m/  Parsed_regularized.txt  resultado.txt  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","source":["# Clonar el Repo\n","! git clone https://huggingface.co/plncmm/guanaco-lora-7b\n","#Inspeccionar la carpeta del repositorio\n","%cd guanaco-lora-7b\n","#Listar los contenidos\n","%ls\n","\n","%cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXSkqzfqPdHg","executionInfo":{"status":"ok","timestamp":1686593220584,"user_tz":300,"elapsed":2237,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"014c91eb-55e8-4f9d-9f12-71b9e536e9b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'guanaco-lora-7b'...\n","remote: Enumerating objects: 11, done.\u001b[K\n","remote: Counting objects: 100% (11/11), done.\u001b[K\n","remote: Compressing objects: 100% (10/10), done.\u001b[K\n","remote: Total 11 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (11/11), 2.76 KiB | 1.38 MiB/s, done.\n","/content/guanaco-lora-7b\n","adapter_config.json  adapter_model.bin  README.md\n","/content\n"]}]},{"cell_type":"code","source":["%cd alpaca-lora"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NpE3rFG6yn1t","executionInfo":{"status":"ok","timestamp":1686593258233,"user_tz":300,"elapsed":438,"user":{"displayName":"Brayan Rolando Jr Portela Cabrera","userId":"01617837405862704789"}},"outputId":"c966b654-990f-4c60-930e-5c5efff016f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/alpaca-lora\n"]}]},{"cell_type":"code","source":["! python finetune.py --base_model 'decapoda-research/llama-7b-hf' --data_path 'yahma/alpaca-cleaned' --output_dir './lora-alpaca' --batch_size 128 --num_epochs 3 --learning_rate 1e-4"],"metadata":{"id":"GBIqFHW4H0ij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd content"],"metadata":{"id":"41xkUOKMVPsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Correr el modelo\n","%cd alpaca-lora\n","\n","! python generate.py --load_8bit --base_model 'decapoda-research/llama-7b-hf' --lora_weights '/content/guanaco-lora-7b'"],"metadata":{"id":"826doOEcEKHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Base model LLaMa-7B\n","base_model_path = 'decapoda-research/llama-7b-hf'\n","# Weights Lora-7B fine tuned for Spanish\n","weights_path = \"/content/guanaco-lora-7b\""],"metadata":{"id":"h2Lg8JwwNZk7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Carga y procesamiento de documentos:\n","\n","* Se cargan los documentos de las resoluciones haciendo uso de la importacón **TextLoader de Langchain**.\n","\n","* Con la cual podremos cargar el documento obtenido en la carga el cual se encuentra regualrizado a partir del diccionario inicial.\n","\n","* Se separa el texto usando un separador recursivo, con el cual partimos el texto original en pequeños trosos con el fin de encontrar los que sean mas relevantes para el LLM."],"metadata":{"id":"pOOyaSq7-1Kp"}},{"cell_type":"code","source":["%cd .."],"metadata":{"id":"5gwoYX4DNoWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%ls"],"metadata":{"id":"gsYdbqpvNwL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import langchain\n","from langchain import text_splitter\n","from langchain.document_loaders import TextLoader\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.chains import ConversationalRetrievalChain\n","\n","loader = TextLoader('/content/Parsed_regularized.txt')\n","ResolutionDoc = loader.load()\n","\n","text_splitter = CharacterTextSplitter(chunk_size =256, chunk_overlap=0)\n","text = text_splitter.split_documents(documents=ResolutionDoc)\n","\n","#Carga del texto\n","print(text)"],"metadata":{"id":"1WZOtnLk-vD7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","### 2. Inicializar una base de vectores persistente de ChromaDB\n","\n","* Ya que necesitamos una base de vectores enlazados para cada trozo de texto en una base de datos Chroma, utilizaremos un directorio para que la base de datos sea Persistente.\n","\n","* Debemos utilizar diferentes embeddings ya que necesitamos representar los datos de una forma en la cual la I.A por lo cual importamos embbedings compatibles con chroma."],"metadata":{"id":"PNVB9r7_-vwQ"}},{"cell_type":"code","source":["!pip install llama-cpp-python"],"metadata":{"id":"5_nSxuAALDG4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceEmbeddings\n","\n","embeding_model_path = \"all-MiniLM-L6-v2\"\n","embeddings = HuggingFaceEmbeddings(model_name=embeding_model_path)"],"metadata":{"id":"bfeCpFdAeShK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.vectorstores import Chroma\n","#from langchain.embeddings import LlamaCppEmbeddings\n","#from chromadb.utils import embedding_functions\n","\n","\n","#llm = LlamaCppEmbeddings(model_path=base_model)\n","persistency_dir = 'chromaDb'\n","#emb_function = LlamaCppEmbeddings()\n","\n","chromaVectorDB = Chroma.from_documents(documents=ResolutionDoc, embedding=embeddings,persist_directory=persistency_dir)\n"],"metadata":{"id":"qpIlkcZ-Fhd-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"T4zYlTkPjmDH"}},{"cell_type":"code","source":["from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n","from langchain.llms import HuggingFacePipeline\n","from langchain import PromptTemplate, LLMChain\n","\n","tokenizer = LlamaTokenizer.from_pretrained(base_model_path)"],"metadata":{"id":"RfxB5-wccKcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model = LlamaForCausalLM.from_pretrained(\n","        base_model_path,\n","        load_in_8bit=True,\n","        device_map=\"cuda:0\",\n","    )"],"metadata":{"id":"SlGPLgyOaZ8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adapt the base model weights\n","from peft import PeftModel\n","model = PeftModel.from_pretrained(\n","    base_model,\n","    weights_path,\n",")"],"metadata":{"id":"xPisANUubzRE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe = pipeline(\n","    \"text-generation\",\n","    model=base_model,\n","    tokenizer=tokenizer,\n","    max_length=10000,\n","    temperature=0.1,\n","    top_p=0.75,\n","    top_k=40,\n","    repetition_penalty=1.2\n",")\n","\n","local_llm = HuggingFacePipeline(pipeline=pipe)"],"metadata":{"id":"uEEYz3JIk-r4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain import PromptTemplate, LLMChain\n","\n","template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{instruction}\n","\n","Answer:\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"instruction\"])"],"metadata":{"id":"wq1pK8HGmuSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm_chain = LLMChain(prompt=prompt,\n","                     llm=local_llm\n","                     )\n","\n","question = \"Que es un guanaco?\"\n","\n","print(llm_chain.run(question))"],"metadata":{"id":"XRBBGDKDmyE0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chromaVectorDB.persist()\n","chromaVectorDB = None\n","\n","vectordb = Chroma(persist_directory=persistency_dir, embedding_function=embeddings)"],"metadata":{"id":"YkUX1lRjr4Oq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chains import VectorDBQA\n","from langchain.chains import RetrievalQA\n","\n","docsearch = Chroma.from_documents(text, embeddings)\n","Rqa = RetrievalQA.from_chain_type(llm=local_llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}))\n","\n","#Rqa = RetrievalQAWithSourcesChain.from_chain_type(llm=local_llm, chain_type=\"stuff\", retriever=chromaVectorDB.as)\n","Vqa = VectorDBQA.from_chain_type(llm=local_llm, chain_type=\"stuff\", vectorstore=vectordb)"],"metadata":{"id":"oXPCGppNrP7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()\n","\n","import gc\n","gc.collect()"],"metadata":{"id":"rhyo6y--vCvH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","query = \"Que dice la normativa de admision a la Universidad Nacional de Colombia?\"\n","Vqa.run(query)\n","\n"],"metadata":{"id":"px8TxgR9ti60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chains import RetrievalQAWithSourcesChain\n","\n","chain = RetrievalQAWithSourcesChain.from_chain_type(local_llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}))\n","\n","chain({\"question\": \"Que dice la normativa de admision a la Universidad Nacional de Colombia\"}, return_only_outputs=True)"],"metadata":{"id":"iRcOjnq2vWRI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Rqa.run(query)"],"metadata":{"id":"MaK6vjjluqAt"},"execution_count":null,"outputs":[]}]}